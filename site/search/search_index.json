{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Retrieval-Augmented Generation (RAG) with Hugging Face and Gemini","text":"<p>Welcome to the RAG Workshop documentation! This guide will walk you through building a simple Retrieval-Augmented Generation (RAG) system using Hugging Face and Google's Gemini model.</p>"},{"location":"#overview","title":"Overview","text":"<p>RAG enhances the capabilities of Large Language Models (LLMs) by providing them with external information during the text generation process.</p>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<ol> <li>Retrieval: When you ask a question (a \"query\"), the system searches a knowledge base (in our case, the text from a PDF you upload) to find the most relevant snippets of text.</li> <li>Augmentation: These relevant text snippets are then added to your original query to form a new, more detailed prompt.</li> <li>Generation: This augmented prompt is sent to an LLM (like Google's Gemini), which then generates an answer based on the provided context.</li> </ol> <p>This process allows the LLM to answer questions about specific documents it wasn't originally trained on.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started with the workshop, you'll need:</p> <ol> <li>A Google Gemini API key</li> <li>Python 3.7 or later</li> <li>Required packages:<ul> <li>google-generativeai</li> <li>pypdf</li> <li>sentence-transformers</li> <li>faiss-cpu</li> <li>gradio</li> </ul> </li> </ol>"},{"location":"#installation","title":"Installation","text":"<p>Install the required packages using pip:</p> <pre><code>pip install google-generativeai pypdf sentence-transformers faiss-cpu gradio\n</code></pre>"},{"location":"code-walkthrough/","title":"RAG Workshop Implementation Walkthrough","text":"<p>This document provides a detailed walkthrough of the RAG system implementation from the notebook.</p>"},{"location":"code-walkthrough/#1-dependencies-installation","title":"1. Dependencies Installation","text":"<p>The notebook implements a RAG system that combines Google's Gemini model with local document processing. The required dependencies are:</p> <pre><code>!pip install -q google-generativeai pypdf sentence-transformers faiss-cpu\n</code></pre> <p>Each library serves a specific purpose: - <code>google-generativeai</code>: Interface with Gemini model - <code>pypdf</code>: PDF document processing - <code>sentence-transformers</code>: Text embedding generation - <code>faiss-cpu</code>: Vector similarity search - <code>gradio</code>: Web interface creation</p>"},{"location":"code-walkthrough/#2-api-configuration","title":"2. API Configuration","text":"<pre><code>import os\nimport getpass\nimport google.generativeai as genai\n\n# Get API key securely\nif \"GEMINI_API_KEY\" not in os.environ:\n    os.environ[\"GEMINI_API_KEY\"] = getpass.getpass(\"Enter your Gemini API key: \")\n\n# Configure Gemini\ngenai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\nmodel = genai.GenerativeModel('gemini-2.5-flash')\n</code></pre>"},{"location":"code-walkthrough/#key-points","title":"Key Points:","text":"<ul> <li>Uses environment variables for API key</li> <li>Secure input with getpass</li> <li>Initializes Gemini model</li> <li>Uses 'gemini-2.5-flash' for faster responses</li> </ul>"},{"location":"code-walkthrough/#3-document-processing","title":"3. Document Processing","text":"<pre><code>from google.colab import files\nfrom pypdf import PdfReader\n\ndef process_pdf(file):\n    \"\"\"Process uploaded PDF file.\"\"\"\n    reader = PdfReader(file)\n    text = \"\"\n    for page in reader.pages:\n        text += page.extract_text()\n    return text\n\ndef create_chunks(text, chunk_size=1000):\n    \"\"\"Split text into chunks.\"\"\"\n    return [text[i:i + chunk_size] \n            for i in range(0, len(text), chunk_size)]\n</code></pre>"},{"location":"code-walkthrough/#implementation-details","title":"Implementation Details:","text":"<ul> <li>Uses PdfReader for text extraction</li> <li>Simple chunking strategy (1000 characters)</li> <li>No overlap between chunks</li> <li>Handles PDF upload through Colab interface</li> </ul>"},{"location":"code-walkthrough/#improvements-possible","title":"Improvements Possible:","text":"<ul> <li>Add chunk overlap</li> <li>Implement smarter chunking (sentence boundaries)</li> <li>Add metadata tracking</li> <li>Handle different file types</li> </ul>"},{"location":"code-walkthrough/#4-embedding-generation","title":"4. Embedding Generation","text":"<pre><code>from sentence_transformers import SentenceTransformer\n\n# Initialize embedding model\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n\ndef generate_embeddings(chunks):\n    \"\"\"Generate embeddings for text chunks.\"\"\"\n    return embedding_model.encode(chunks)\n</code></pre>"},{"location":"code-walkthrough/#model-choice","title":"Model Choice:","text":"<ul> <li>all-MiniLM-L6-v2: Good balance of speed and quality</li> <li>384-dimensional embeddings</li> <li>Optimized for semantic similarity</li> </ul>"},{"location":"code-walkthrough/#process-flow","title":"Process Flow:","text":"<ol> <li>Load pre-trained model</li> <li>Convert text chunks to embeddings</li> <li>Return numpy array of embeddings</li> </ol>"},{"location":"code-walkthrough/#5-vector-store-setup","title":"5. Vector Store Setup","text":"<pre><code>import faiss\nimport numpy as np\n\ndef create_vector_store(embeddings):\n    \"\"\"Create FAISS index for embeddings.\"\"\"\n    # Get embedding dimension\n    dimension = embeddings.shape[1]\n\n    # Create L2 index\n    index = faiss.IndexFlatL2(dimension)\n\n    # Add vectors\n    index.add(np.array(embeddings).astype('float32'))\n\n    return index\n</code></pre>"},{"location":"code-walkthrough/#faiss-configuration","title":"FAISS Configuration:","text":"<ul> <li>Uses L2 distance metric</li> <li>Flat index for exact search</li> <li>float32 data type for vectors</li> <li>No quantization or clustering</li> </ul>"},{"location":"code-walkthrough/#design-choices","title":"Design Choices:","text":"<ul> <li>Simple IndexFlatL2 for small datasets</li> <li>Could use IVF or HNSW for scaling</li> <li>Direct memory storage (no persistence)</li> </ul>"},{"location":"code-walkthrough/#6-rag-implementation","title":"6. RAG Implementation","text":"<pre><code>def get_rag_answer(query, k=3):\n    \"\"\"\n    Performs retrieval-augmented generation.\n\n    Args:\n        query: User question\n        k: Number of chunks to retrieve\n\n    Returns:\n        Generated answer\n    \"\"\"\n    # 1. Retrieval\n    query_embedding = embedding_model.encode([query])\n    distances, indices = index.search(\n        np.array(query_embedding).astype('float32'),\n        k\n    )\n\n    # 2. Augmentation\n    retrieved_chunks = [text_chunks[i] for i in indices[0]]\n    context = \"\\n\\n\".join(retrieved_chunks)\n\n    # 3. Prompt Construction\n    prompt = f\"\"\"Based on the following context, answer the question.\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n\n    # 4. Generation\n    response = model.generate_content(prompt)\n    return response.text\n</code></pre>"},{"location":"code-walkthrough/#function-components","title":"Function Components:","text":"<ol> <li>Query Processing:</li> <li>Converts query to embedding</li> <li>Same model as document embeddings</li> <li> <p>Single vector output</p> </li> <li> <p>Retrieval:</p> </li> <li>Uses FAISS k-nearest neighbors</li> <li>Returns top-k most similar chunks</li> <li> <p>Includes distance scores</p> </li> <li> <p>Context Assembly:</p> </li> <li>Combines retrieved chunks</li> <li>Simple concatenation with newlines</li> <li> <p>No context ordering/ranking</p> </li> <li> <p>Prompt Engineering:</p> </li> <li>Clear instruction format</li> <li>Explicit context separation</li> <li> <p>Simple question-answer structure</p> </li> <li> <p>Generation:</p> </li> <li>Uses Gemini model</li> <li>No temperature control</li> <li>No response formatting</li> </ol>"},{"location":"code-walkthrough/#design-considerations","title":"Design Considerations:","text":"<ul> <li>k=3 balances context size and relevance</li> <li>Simple prompt template for reliability</li> <li>No streaming or async processing</li> <li>No error handling for long inputs</li> </ul>"},{"location":"code-walkthrough/#7-user-interface","title":"7. User Interface","text":"<pre><code>import gradio as gr\n\ndef chat_with_pdf(question, history):\n    \"\"\"Handle chat interface.\"\"\"\n    if not question.strip():\n        return \"Please enter a question.\"\n\n    try:\n        return get_rag_answer(question)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Create interface\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    # Header\n    gr.Markdown(\"# \ud83d\udcda RAG-Powered PDF Q&amp;A System\")\n\n    # Input/Output\n    with gr.Row():\n        with gr.Column():\n            question_input = gr.Textbox(\n                label=\"Your Question\",\n                lines=3\n            )\n            submit_btn = gr.Button(\"Ask\")\n\n        with gr.Column():\n            answer_output = gr.Textbox(\n                label=\"Answer\",\n                lines=10\n            )\n\n    # Example questions\n    gr.Examples([\n        [\"What is the main topic?\"],\n        [\"Summarize key points\"],\n        [\"What details are mentioned?\"]\n    ], inputs=question_input)\n\n    # Event handlers\n    submit_btn.click(\n        fn=lambda q: chat_with_pdf(q, None),\n        inputs=question_input,\n        outputs=answer_output\n    )\n</code></pre>"},{"location":"code-walkthrough/#ui-components","title":"UI Components:","text":"<ol> <li> <p>Layout:</p> <ul> <li>Two-column design</li> <li>Clean, modern theme</li> <li>Responsive layout</li> </ul> </li> <li> <p>Input Features:</p> <ul> <li>Multi-line question input</li> <li>Submit button</li> <li>Example questions</li> </ul> </li> <li> <p>Output Display:</p> <ul> <li>Multi-line answer box</li> <li>Error handling</li> <li>Clear formatting</li> </ul> </li> <li> <p>Interactions:</p> <ul> <li>Click and submit events</li> <li>Simple callback structure</li> <li>No state management</li> </ul> </li> </ol>"},{"location":"code-walkthrough/#implementation-notes","title":"Implementation Notes:","text":"<ul> <li>Uses Gradio Blocks for flexibility</li> <li>No conversation history</li> <li>Basic error handling</li> <li>Example questions for guidance</li> </ul>"},{"location":"code-walkthrough/#8-current-implementation-features","title":"8. Current Implementation Features","text":"<ol> <li> <p>Document Processing:</p> <ul> <li>Full PDF text extraction</li> <li>Fixed-size chunk splitting (1000 chars)</li> <li>Sequential page processing</li> </ul> </li> <li> <p>Retrieval System:</p> <ul> <li>Exact vector similarity search</li> <li>Top-3 chunk retrieval</li> <li>L2 distance metric</li> </ul> </li> <li> <p>Question Answering:</p> <ul> <li>Context-aware responses</li> <li>Basic error handling</li> <li>Example questions provided</li> </ul> </li> <li> <p>User Interface:</p> <ul> <li>Two-column layout</li> <li>Responsive design</li> <li>Input validation</li> <li>Debug mode enabled</li> </ul> </li> </ol>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>This guide explains all the configurable parameters and settings in the RAG Workshop project.</p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file in your project root with the following settings:</p> <pre><code># API Keys\nGEMINI_API_KEY=your_gemini_api_key_here\n\n# Optional: Model Settings\nEMBEDDING_MODEL=all-MiniLM-L6-v2\nCHUNK_SIZE=1000\nTOP_K=3\n</code></pre>"},{"location":"configuration/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"configuration/#document-processing","title":"Document Processing","text":"Parameter Default Description <code>CHUNK_SIZE</code> 1000 Number of characters per text chunk <code>CHUNK_OVERLAP</code> 200 Overlap between consecutive chunks <code>CLEAN_TEXT</code> True Whether to clean extracted text"},{"location":"configuration/#embedding-generation","title":"Embedding Generation","text":"Parameter Default Description <code>EMBEDDING_MODEL</code> 'all-MiniLM-L6-v2' SentenceTransformer model name <code>EMBEDDING_DIMENSION</code> 384 Embedding vector size <code>NORMALIZE_EMBEDDINGS</code> True Whether to normalize vectors"},{"location":"configuration/#vector-store","title":"Vector Store","text":"Parameter Default Description <code>INDEX_TYPE</code> 'L2' FAISS index type (L2 or IP) <code>NLIST</code> 100 Number of cells for IVF index <code>NPROBE</code> 10 Number of cells to probe"},{"location":"configuration/#retrieval","title":"Retrieval","text":"Parameter Default Description <code>TOP_K</code> 3 Number of chunks to retrieve <code>MIN_SIMILARITY</code> 0.6 Minimum similarity threshold <code>MAX_CONTEXT_LENGTH</code> 2000 Maximum context window size"},{"location":"configuration/#llm-generation","title":"LLM Generation","text":"Parameter Default Description <code>MODEL_NAME</code> 'gemini-2.5-flash' Gemini model version <code>TEMPERATURE</code> 0.7 Generation temperature <code>MAX_TOKENS</code> 1000 Maximum response length"},{"location":"configuration/#custom-configuration","title":"Custom Configuration","text":"<p>You can create a <code>config.yaml</code> file for custom settings:</p> <pre><code>document_processing:\n  chunk_size: 1000\n  chunk_overlap: 200\n  clean_text: true\n\nembedding:\n  model_name: all-MiniLM-L6-v2\n  dimension: 384\n  normalize: true\n\nvector_store:\n  index_type: L2\n  nlist: 100\n  nprobe: 10\n\nretrieval:\n  top_k: 3\n  min_similarity: 0.6\n  max_context_length: 2000\n\nllm:\n  model_name: gemini-2.5-flash\n  temperature: 0.7\n  max_tokens: 1000\n</code></pre>"},{"location":"configuration/#loading-configuration","title":"Loading Configuration","text":"<pre><code>import yaml\n\ndef load_config(config_path=\"config.yaml\"):\n    with open(config_path, 'r') as f:\n        return yaml.safe_load(f)\n\nconfig = load_config()\n</code></pre>"},{"location":"configuration/#environment-setup","title":"Environment Setup","text":"<ol> <li> <p>Copy the <code>.env.example</code> to <code>.env</code>:    <pre><code>cp .env.example .env\n</code></pre></p> </li> <li> <p>Edit the <code>.env</code> file with your API keys and settings</p> </li> <li> <p>Load environment variables in your code:    <pre><code>from dotenv import load_dotenv\nload_dotenv()\n</code></pre></p> </li> </ol>"},{"location":"configuration/#configuration-best-practices","title":"Configuration Best Practices","text":"<ol> <li> <p>API Keys</p> <ul> <li>Never commit API keys to version control</li> <li>Use environment variables for sensitive data</li> <li>Rotate keys regularly</li> </ul> </li> <li> <p>Model Settings</p> <ul> <li>Start with default values</li> <li>Adjust based on your specific use case</li> <li>Monitor performance metrics</li> </ul> </li> <li> <p>Resource Usage</p> <ul> <li>Balance chunk size with memory usage</li> <li>Adjust vector store parameters for scale</li> <li>Optimize context window size</li> </ul> </li> <li> <p>Performance Tuning</p> <ul> <li>Monitor retrieval accuracy</li> <li>Adjust similarity thresholds</li> <li>Fine-tune generation parameters</li> </ul> </li> </ol>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you set up and run the RAG Workshop project quickly.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/#python-environment","title":"Python Environment","text":"<ul> <li>Python 3.7 or later</li> <li>Virtual environment (conda or venv) recommended</li> </ul>"},{"location":"getting-started/#required-libraries","title":"Required Libraries","text":"<ul> <li><code>google-generativeai</code>: Google's Gemini API client</li> <li><code>pypdf</code>: PDF processing</li> <li><code>sentence-transformers</code>: Document embedding generation</li> <li><code>faiss-cpu</code>: Vector similarity search</li> <li><code>gradio</code>: Web interface creation</li> </ul>"},{"location":"getting-started/#api-keys","title":"API Keys","text":"<ol> <li>Google Gemini API Key</li> <li>Visit Google AI Studio</li> <li>Create a new API key</li> <li>Save it securely</li> </ol>"},{"location":"getting-started/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/yourusername/rag-workshop.git\ncd rag-workshop\n</code></pre>"},{"location":"getting-started/#2-create-a-virtual-environment","title":"2. Create a Virtual Environment","text":"<p>=== \"Using venv\"     <pre><code>python -m venv venv\n.\\venv\\Scripts\\activate  # Windows\nsource venv/bin/activate  # Linux/Mac\n</code></pre></p> <p>=== \"Using conda\"     <pre><code>conda create -n rag-workshop python=3.9\nconda activate rag-workshop\n</code></pre></p>"},{"location":"getting-started/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/#4-set-up-environment-variables","title":"4. Set Up Environment Variables","text":"<p>Create a <code>.env</code> file in your project root: <pre><code>GEMINI_API_KEY=your_api_key_here\n</code></pre></p>"},{"location":"getting-started/#quick-start-example","title":"Quick Start Example","text":""},{"location":"getting-started/#1-launch-jupyter-notebook","title":"1. Launch Jupyter Notebook","text":"<pre><code>jupyter notebook rag_workshop.ipynb\n</code></pre>"},{"location":"getting-started/#2-run-the-example","title":"2. Run the Example","text":"<p>Execute the following code in your notebook:</p> <pre><code>import os\nimport google.generativeai as genai\nfrom dotenv import load_dotenv\n\n# Load API key\nload_dotenv()\ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n# Initialize model\nmodel = genai.GenerativeModel('gemini-2.5-flash')\n\n# Test the setup\nresponse = model.generate_content(\"Hello! Can you verify that you're working?\")\nprint(response.text)\n</code></pre> <p>If you see a response from the model, congratulations! Your setup is complete.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about the Project Overview</li> <li>Explore the Code Walkthrough</li> <li>Try out the Examples</li> </ul>"},{"location":"overview/","title":"Project Overview","text":""},{"location":"overview/#what-is-rag","title":"What is RAG?","text":"<p>Retrieval-Augmented Generation (RAG) is a powerful technique that enhances Large Language Models (LLMs) by providing them with relevant context from a knowledge base during the generation process. Instead of relying solely on the model's training data, RAG allows the model to reference specific documents or data sources when answering questions.</p>"},{"location":"overview/#how-rag-works","title":"How RAG Works","text":""},{"location":"overview/#1-document-processing","title":"1. Document Processing","text":"<ul> <li>Documents are split into manageable chunks</li> <li>Each chunk is converted into a vector embedding</li> <li>Embeddings are stored in a vector database</li> </ul>"},{"location":"overview/#2-query-processing","title":"2. Query Processing","text":"<ul> <li>User question is converted to an embedding</li> <li>Similar document chunks are retrieved</li> <li>Retrieved context is combined with the question</li> </ul>"},{"location":"overview/#3-answer-generation","title":"3. Answer Generation","text":"<ul> <li>Enhanced prompt is sent to the LLM</li> <li>Model generates response using provided context</li> <li>Answer is returned to the user</li> </ul>"},{"location":"overview/#system-architecture","title":"System Architecture","text":"<pre><code>graph TD\n    A[Document Input] --&gt; B[Document Processing]\n    B --&gt; C[Text Chunking]\n    C --&gt; D[Embedding Generation]\n    D --&gt; E[Vector Store]\n\n    F[User Query] --&gt; G[Query Embedding]\n    G --&gt; H[Similarity Search]\n    E --&gt; H\n    H --&gt; I[Context Retrieval]\n    I --&gt; J[Prompt Construction]\n    J --&gt; K[LLM Generation]\n    K --&gt; L[Response]</code></pre>"},{"location":"overview/#pipeline-components","title":"Pipeline Components","text":""},{"location":"overview/#1-data-ingestion","title":"1. Data Ingestion","text":"<ul> <li>PDF document loading</li> <li>Text extraction</li> <li>Chunk size optimization</li> </ul>"},{"location":"overview/#2-embedding-generation","title":"2. Embedding Generation","text":"<ul> <li>Sentence Transformer model</li> <li>Semantic text representation</li> <li>Dimension reduction techniques</li> </ul>"},{"location":"overview/#3-vector-store","title":"3. Vector Store","text":"<ul> <li>FAISS index creation</li> <li>Efficient similarity search</li> <li>Memory optimization</li> </ul>"},{"location":"overview/#4-retrieval-system","title":"4. Retrieval System","text":"<ul> <li>Top-k retrieval</li> <li>Context window management</li> <li>Relevance scoring</li> </ul>"},{"location":"overview/#5-llm-integration","title":"5. LLM Integration","text":"<ul> <li>Google Gemini model</li> <li>Prompt engineering</li> <li>Response generation</li> </ul>"},{"location":"overview/#key-features","title":"Key Features","text":""},{"location":"overview/#document-processing","title":"Document Processing","text":"<ul> <li>Supports PDF documents</li> <li>Configurable chunk sizes</li> <li>Automatic text cleaning</li> </ul>"},{"location":"overview/#embedding-system","title":"Embedding System","text":"<ul> <li>State-of-the-art models</li> <li>Optimized for semantic search</li> <li>Fast vector operations</li> </ul>"},{"location":"overview/#retrieval-engine","title":"Retrieval Engine","text":"<ul> <li>Efficient similarity search</li> <li>Configurable retrieval count</li> <li>Context relevance scoring</li> </ul>"},{"location":"overview/#user-interface","title":"User Interface","text":"<ul> <li>Interactive Gradio web UI</li> <li>Real-time responses</li> <li>PDF upload capability</li> </ul>"},{"location":"overview/#use-cases","title":"Use Cases","text":"<ol> <li> <p>Document Q&amp;A</p> <ul> <li>Answer questions about specific documents</li> <li>Extract relevant information</li> <li>Summarize content</li> </ul> </li> <li> <p>Knowledge Base Search</p> <ul> <li>Find relevant information quickly</li> <li>Connect related concepts</li> <li>Provide contextual answers</li> </ul> </li> <li> <p>Information Retrieval</p> <ul> <li>Search through large documents</li> <li>Find specific details</li> <li>Extract key information</li> </ul> </li> </ol>"},{"location":"overview/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Getting Started Guide</li> <li>Explore the Code Walkthrough</li> <li>Try the Examples</li> </ul>"},{"location":"workshop/","title":"Workshop Implementation","text":"<p>This page provides a detailed walkthrough of implementing the RAG system.</p>"},{"location":"workshop/#1-configure-api-key","title":"1. Configure API Key","text":"<p>First, you'll need to configure your Gemini API key:</p> <pre><code>import os\nimport getpass\nimport google.generativeai as genai\n\n# Configure the Gemini API\nos.environ[\"GEMINI_API_KEY\"] = \"your-api-key-here\"\ngenai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n\n# Initialize the generative model\nmodel = genai.GenerativeModel('gemini-2.5-flash')\n</code></pre>"},{"location":"workshop/#2-process-pdf-documents","title":"2. Process PDF Documents","text":"<p>The system can process PDF documents to create a knowledge base:</p> <pre><code>from pypdf import PdfReader\n\ndef process_pdf(pdf_path):\n    reader = PdfReader(pdf_path)\n    pdf_text = \"\"\n    for page in reader.pages:\n        pdf_text += page.extract_text()\n\n    # Split the text into chunks\n    text_chunks = [pdf_text[i:i + 1000] for i in range(0, len(pdf_text), 1000)]\n    return text_chunks\n</code></pre>"},{"location":"workshop/#3-create-text-embeddings","title":"3. Create Text Embeddings","text":"<p>We use Sentence Transformers to create embeddings:</p> <pre><code>from sentence_transformers import SentenceTransformer\n\n# Load a pre-trained model from Hugging Face\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Create embeddings for the text chunks\nembeddings = embedding_model.encode(text_chunks)\n</code></pre>"},{"location":"workshop/#4-build-vector-store","title":"4. Build Vector Store","text":"<p>FAISS is used to create a vector store for efficient similarity search:</p> <pre><code>import faiss\nimport numpy as np\n\n# Create a FAISS index\nd = embeddings.shape[1]  # dimension of the embeddings\nindex = faiss.IndexFlatL2(d)\nindex.add(np.array(embeddings).astype('float32'))\n</code></pre>"},{"location":"workshop/#5-rag-implementation","title":"5. RAG Implementation","text":"<p>The core RAG functionality:</p> <pre><code>def get_rag_answer(query, k=3):\n    \"\"\"\n    Performs retrieval-augmented generation.\n\n    Args:\n        query: The user's question\n        k: Number of relevant chunks to retrieve\n    \"\"\"\n    # 1. Retrieval\n    query_embedding = embedding_model.encode([query])\n    distances, indices = index.search(np.array(query_embedding).astype('float32'), k)\n\n    # Get relevant text chunks\n    retrieved_chunks = [text_chunks[i] for i in indices[0]]\n\n    # 2. Augmentation\n    context = \"\\n\\n\".join(retrieved_chunks)\n    augmented_prompt = f\"\"\"Based on the following context, answer the question.\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n\n    # 3. Generation\n    response = model.generate_content(augmented_prompt)\n    return response.text\n</code></pre>"},{"location":"workshop/#6-interactive-interface","title":"6. Interactive Interface","text":"<p>The system includes a Gradio-based interface for easy interaction:</p> <p>```python import gradio as gr</p> <p>def chat_with_pdf(question):     if not question.strip():         return \"Please enter a question.\"</p> <pre><code>try:\n    answer = get_rag_answer(question)\n    return answer\nexcept Exception as e:\n    return f\"Error: {str(e)}\"\n</code></pre>"},{"location":"workshop/#create-gradio-interface","title":"Create Gradio interface","text":"<p>demo = gr.Interface(     fn=chat_with_pdf,     inputs=gr.Textbox(lines=3, label=\"Your Question\"),     outputs=gr.Textbox(lines=5, label=\"Answer\") )</p>"},{"location":"code/data-loading/","title":"Data Loading","text":"<p>The first step in our RAG pipeline is loading and processing documents. This guide explains how we handle document ingestion and preprocessing.</p>"},{"location":"code/data-loading/#pdf-document-loading","title":"PDF Document Loading","text":"<p>We use the <code>pypdf</code> library to load and extract text from PDF documents:</p> <pre><code>from pypdf import PdfReader\n\ndef load_pdf(file_path):\n    \"\"\"\n    Load and extract text from a PDF file.\n\n    Args:\n        file_path (str): Path to the PDF file\n\n    Returns:\n        str: Extracted text from the PDF\n    \"\"\"\n    reader = PdfReader(file_path)\n    pdf_text = \"\"\n\n    # Extract text from each page\n    for page in reader.pages:\n        pdf_text += page.extract_text()\n\n    return pdf_text\n</code></pre>"},{"location":"code/data-loading/#text-chunking","title":"Text Chunking","text":"<p>After extracting text, we split it into manageable chunks:</p> <pre><code>def create_chunks(text, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Split text into overlapping chunks.\n\n    Args:\n        text (str): Input text to chunk\n        chunk_size (int): Size of each chunk\n        chunk_overlap (int): Overlap between chunks\n\n    Returns:\n        list: List of text chunks\n    \"\"\"\n    chunks = []\n    start = 0\n\n    while start &lt; len(text):\n        # Calculate end position with overlap\n        end = start + chunk_size\n\n        # Add chunk to list\n        chunk = text[start:end]\n        chunks.append(chunk)\n\n        # Move start position, accounting for overlap\n        start = end - chunk_overlap\n\n    return chunks\n</code></pre>"},{"location":"code/data-loading/#text-preprocessing","title":"Text Preprocessing","text":"<p>Before chunking, we clean and normalize the text:</p> <pre><code>import re\n\ndef clean_text(text):\n    \"\"\"\n    Clean and normalize text.\n\n    Args:\n        text (str): Input text\n\n    Returns:\n        str: Cleaned text\n    \"\"\"\n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text)\n\n    # Remove special characters\n    text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n\n    # Normalize whitespace\n    text = text.strip()\n\n    return text\n</code></pre>"},{"location":"code/data-loading/#complete-pipeline","title":"Complete Pipeline","text":"<p>Here's how to use all components together:</p> <pre><code>def process_document(file_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Process a PDF document from start to finish.\n\n    Args:\n        file_path (str): Path to PDF file\n        chunk_size (int): Size of each chunk\n        chunk_overlap (int): Overlap between chunks\n\n    Returns:\n        list: Processed text chunks\n    \"\"\"\n    # Load PDF\n    raw_text = load_pdf(file_path)\n\n    # Clean text\n    cleaned_text = clean_text(raw_text)\n\n    # Create chunks\n    chunks = create_chunks(\n        cleaned_text,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n\n    return chunks\n</code></pre>"},{"location":"code/data-loading/#usage-example","title":"Usage Example","text":"<pre><code># Process a PDF document\nfile_path = \"example.pdf\"\nchunks = process_document(\n    file_path,\n    chunk_size=1000,\n    chunk_overlap=200\n)\n\nprint(f\"Created {len(chunks)} chunks\")\n\n# Preview first chunk\nprint(\"\\nFirst chunk preview:\")\nprint(chunks[0][:200] + \"...\")\n</code></pre>"},{"location":"code/data-loading/#configuration","title":"Configuration","text":"<p>You can customize the processing parameters in your config file:</p> <pre><code>document_processing:\n  chunk_size: 1000\n  chunk_overlap: 200\n  clean_text: true\n</code></pre>"},{"location":"code/data-loading/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Chunk Size</p> <ul> <li>Experiment with different sizes</li> <li>Consider model context limits</li> <li>Balance detail vs. context</li> </ul> </li> <li> <p>Text Cleaning</p> <ul> <li>Preserve meaningful punctuation</li> <li>Remove irrelevant characters</li> <li>Maintain sentence structure</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Validate PDF files</li> <li>Handle encoding issues</li> <li>Log processing errors</li> </ul> </li> </ol>"},{"location":"code/data-loading/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Embedding Generation</li> <li>Explore Vector Store setup</li> <li>See Example Usage</li> </ul>"},{"location":"code/embeddings/","title":"Embedding Generation","text":"<p>This section explains how we generate embeddings from text chunks using Sentence Transformers.</p>"},{"location":"code/embeddings/#overview","title":"Overview","text":"<p>Embeddings are numerical representations of text that capture semantic meaning. We use the <code>sentence-transformers</code> library to convert text chunks into dense vector embeddings.</p>"},{"location":"code/embeddings/#implementation","title":"Implementation","text":""},{"location":"code/embeddings/#setup","title":"Setup","text":"<pre><code>from sentence_transformers import SentenceTransformer\n\ndef initialize_embedding_model(model_name='all-MiniLM-L6-v2'):\n    \"\"\"\n    Initialize the embedding model.\n\n    Args:\n        model_name (str): Name of the pretrained model\n\n    Returns:\n        SentenceTransformer: Initialized model\n    \"\"\"\n    return SentenceTransformer(model_name)\n</code></pre>"},{"location":"code/embeddings/#generating-embeddings","title":"Generating Embeddings","text":"<pre><code>import numpy as np\n\ndef generate_embeddings(texts, model):\n    \"\"\"\n    Generate embeddings for a list of texts.\n\n    Args:\n        texts (list): List of text chunks\n        model (SentenceTransformer): Initialized model\n\n    Returns:\n        np.ndarray: Matrix of embeddings\n    \"\"\"\n    # Generate embeddings\n    embeddings = model.encode(\n        texts,\n        show_progress_bar=True,\n        normalize_embeddings=True\n    )\n\n    return embeddings\n</code></pre>"},{"location":"code/embeddings/#model-selection","title":"Model Selection","text":"<p>We use the <code>all-MiniLM-L6-v2</code> model by default because it:</p> <ol> <li>Provides good performance for most use cases</li> <li>Has a reasonable embedding dimension (384)</li> <li>Is fast and memory-efficient</li> <li>Works well with semantic search</li> </ol>"},{"location":"code/embeddings/#available-models","title":"Available Models","text":"Model Name Dimension Speed Performance all-MiniLM-L6-v2 384 Fast Good all-mpnet-base-v2 768 Medium Better all-roberta-large-v1 1024 Slow Best"},{"location":"code/embeddings/#usage-example","title":"Usage Example","text":"<pre><code># Initialize model\nmodel_name = 'all-MiniLM-L6-v2'\nembedding_model = initialize_embedding_model(model_name)\n\n# Generate embeddings for text chunks\nchunks = [\n    \"This is the first document.\",\n    \"Another document with different content.\",\n    \"A third document for demonstration.\"\n]\n\nembeddings = generate_embeddings(chunks, embedding_model)\n\nprint(f\"Generated {len(embeddings)} embeddings\")\nprint(f\"Embedding dimension: {embeddings.shape[1]}\")\n</code></pre>"},{"location":"code/embeddings/#configuration","title":"Configuration","text":"<p>Customize embedding generation in your config file:</p> <pre><code>embedding:\n  model_name: all-MiniLM-L6-v2\n  normalize: true\n  batch_size: 32\n  show_progress: true\n</code></pre>"},{"location":"code/embeddings/#advanced-usage","title":"Advanced Usage","text":""},{"location":"code/embeddings/#batch-processing","title":"Batch Processing","text":"<p>For large document collections:</p> <pre><code>def batch_generate_embeddings(texts, model, batch_size=32):\n    \"\"\"\n    Generate embeddings in batches.\n\n    Args:\n        texts (list): List of text chunks\n        model (SentenceTransformer): Initialized model\n        batch_size (int): Batch size\n\n    Returns:\n        np.ndarray: Matrix of embeddings\n    \"\"\"\n    embeddings = []\n\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]\n        batch_embeddings = model.encode(\n            batch,\n            show_progress_bar=False,\n            normalize_embeddings=True\n        )\n        embeddings.append(batch_embeddings)\n\n    return np.vstack(embeddings)\n</code></pre>"},{"location":"code/embeddings/#embedding-pooling","title":"Embedding Pooling","text":"<p>Combine multiple embeddings:</p> <pre><code>def pool_embeddings(embeddings, method='mean'):\n    \"\"\"\n    Pool multiple embeddings into one.\n\n    Args:\n        embeddings (np.ndarray): Matrix of embeddings\n        method (str): Pooling method ('mean' or 'max')\n\n    Returns:\n        np.ndarray: Pooled embedding\n    \"\"\"\n    if method == 'mean':\n        return np.mean(embeddings, axis=0)\n    elif method == 'max':\n        return np.max(embeddings, axis=0)\n    else:\n        raise ValueError(\"Invalid pooling method\")\n</code></pre>"},{"location":"code/embeddings/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Model Selection</p> <ul> <li>Choose based on your specific needs</li> <li>Consider speed vs. accuracy tradeoff</li> <li>Test different models for your use case</li> </ul> </li> <li> <p>Memory Management</p> <ul> <li>Use batch processing for large datasets</li> <li>Clear GPU memory when needed</li> <li>Monitor memory usage</li> </ul> </li> <li> <p>Performance Optimization</p> <ul> <li>Normalize embeddings</li> <li>Use appropriate batch sizes</li> <li>Cache embeddings when possible</li> </ul> </li> </ol>"},{"location":"code/embeddings/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Out of Memory</p> <ul> <li>Reduce batch size</li> <li>Use CPU if GPU memory is limited</li> <li>Process in chunks</li> </ul> </li> <li> <p>Slow Processing</p> <ul> <li>Use smaller models</li> <li>Increase batch size</li> <li>Enable GPU acceleration</li> </ul> </li> <li> <p>Quality Issues</p> <ul> <li>Try different models</li> <li>Adjust text preprocessing</li> <li>Validate embedding quality</li> </ul> </li> </ol>"},{"location":"code/embeddings/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Vector Store setup</li> <li>Explore Retrieval methods</li> <li>See Example Usage</li> </ul>"},{"location":"code/generator/","title":"Generator / LLM Integration","text":"<p>This section explains how we integrate with Google's Gemini model for generating answers based on retrieved context.</p>"},{"location":"code/generator/#overview","title":"Overview","text":"<p>The generator component: 1. Takes retrieved context and user query 2. Constructs an appropriate prompt 3. Calls the Gemini API 4. Processes and returns the response</p>"},{"location":"code/generator/#implementation","title":"Implementation","text":""},{"location":"code/generator/#basic-generator","title":"Basic Generator","text":"<pre><code>import google.generativeai as genai\n\nclass Generator:\n    def __init__(self, model_name='gemini-2.5-flash'):\n        self.model = genai.GenerativeModel(model_name)\n\n    def generate(self, query, contexts, temperature=0.3):\n        \"\"\"\n        Generate an answer using retrieved contexts.\n\n        Args:\n            query (str): User question\n            contexts (list): Retrieved text passages\n            temperature (float): Generation temperature\n\n        Returns:\n            str: Generated answer\n        \"\"\"\n        # Build prompt\n        prompt = self._build_prompt(query, contexts)\n\n        # Generate response\n        response = self.model.generate_content(\n            prompt,\n            temperature=temperature\n        )\n\n        return response.text\n\n    def _build_prompt(self, query, contexts):\n        \"\"\"Build a prompt from query and contexts.\"\"\"\n        context_text = \"\\n\\n\".join(\n            f\"[{i+1}] {ctx}\" \n            for i, ctx in enumerate(contexts)\n        )\n\n        return f\"\"\"Based on the following passages, answer the question.\n        If you cannot find relevant information, say so.\n        Include passage numbers [1], [2], etc. to cite your sources.\n\n        Passages:\n        {context_text}\n\n        Question: {query}\n\n        Answer:\"\"\"\n</code></pre>"},{"location":"code/generator/#advanced-generator","title":"Advanced Generator","text":"<pre><code>class AdvancedGenerator(Generator):\n    def __init__(self, model_name='gemini-2.5-flash'):\n        super().__init__(model_name)\n        self.history = []\n\n    def generate(self, query, contexts, \n                temperature=0.3,\n                max_tokens=1000,\n                citation_required=True):\n        \"\"\"\n        Enhanced generation with history and citations.\n        \"\"\"\n        # Build conversation history\n        messages = self._build_conversation(\n            query, contexts, citation_required\n        )\n\n        # Generate with safety settings\n        response = self.model.generate_content(\n            messages,\n            generation_config={\n                'temperature': temperature,\n                'max_output_tokens': max_tokens,\n                'top_p': 0.9,\n                'top_k': 40\n            },\n            safety_settings={\n                'harmful_categories': 'block'\n            }\n        )\n\n        # Update history\n        self.history.append({\n            'query': query,\n            'contexts': contexts,\n            'response': response.text\n        })\n\n        return self._format_response(response.text)\n\n    def _build_conversation(self, query, contexts, \n                          citation_required):\n        \"\"\"Build conversation with history.\"\"\"\n        system_prompt = \"\"\"You are a helpful AI assistant.\n        Answer questions based only on the provided context.\n        Be concise and accurate.\"\"\"\n\n        if citation_required:\n            system_prompt += \"\\nCite passages using [1], [2] etc.\"\n\n        context_text = self._format_contexts(contexts)\n\n        return [\n            {'role': 'system', 'content': system_prompt},\n            {'role': 'user', 'content': f\"Context:\\n{context_text}\"},\n            {'role': 'user', 'content': f\"Question: {query}\"}\n        ]\n</code></pre>"},{"location":"code/generator/#prompt-engineering","title":"Prompt Engineering","text":""},{"location":"code/generator/#basic-prompt-template","title":"Basic Prompt Template","text":"<pre><code>BASIC_PROMPT_TEMPLATE = \"\"\"Answer the question based on these passages:\n\n{contexts}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n</code></pre>"},{"location":"code/generator/#enhanced-prompt-template","title":"Enhanced Prompt Template","text":"<pre><code>ENHANCED_PROMPT_TEMPLATE = \"\"\"You are a helpful AI assistant answering questions based on provided context.\n\nCONTEXT:\n{contexts}\n\nQUESTION: {query}\n\nInstructions:\n- Use ONLY information from the provided context\n- Cite sources using [1], [2], etc.\n- If context doesn't contain relevant info, say \"I cannot answer based on the provided context\"\n- Be concise and direct\n- Format lists and technical terms appropriately\n\nANSWER:\"\"\"\n</code></pre>"},{"location":"code/generator/#configuration","title":"Configuration","text":"<p>Generator settings in <code>config.yaml</code>:</p> <pre><code>generator:\n  model: gemini-2.5-flash\n  temperature: 0.3\n  max_tokens: 1000\n  citation_required: true\n  prompt:\n    template: enhanced\n    system_message: true\n  safety:\n    harmful_content: block\n    unsafe_content: harmless\n</code></pre>"},{"location":"code/generator/#response-processing","title":"Response Processing","text":""},{"location":"code/generator/#citation-extraction","title":"Citation Extraction","text":"<pre><code>import re\n\ndef extract_citations(text):\n    \"\"\"Extract passage citations from response.\"\"\"\n    citations = re.findall(r'\\[(\\d+)\\]', text)\n    return [int(c) for c in citations]\n\ndef validate_citations(text, num_contexts):\n    \"\"\"Validate citation numbers.\"\"\"\n    citations = extract_citations(text)\n    return all(1 &lt;= c &lt;= num_contexts for c in citations)\n</code></pre>"},{"location":"code/generator/#response-formatting","title":"Response Formatting","text":"<pre><code>def format_response(text, contexts):\n    \"\"\"Format response with citations.\"\"\"\n    citations = extract_citations(text)\n    used_contexts = [contexts[i-1] for i in citations]\n\n    return {\n        'answer': text,\n        'citations': citations,\n        'contexts': used_contexts\n    }\n</code></pre>"},{"location":"code/generator/#error-handling","title":"Error Handling","text":"<pre><code>class GeneratorError(Exception):\n    \"\"\"Base class for generator errors.\"\"\"\n    pass\n\nclass ContextError(GeneratorError):\n    \"\"\"Error when context is invalid.\"\"\"\n    pass\n\nclass APIError(GeneratorError):\n    \"\"\"Error when API call fails.\"\"\"\n    pass\n\ndef handle_generation_errors(func):\n    \"\"\"Decorator for error handling.\"\"\"\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except genai.APIError as e:\n            raise APIError(f\"API error: {str(e)}\")\n        except Exception as e:\n            raise GeneratorError(f\"Generation failed: {str(e)}\")\n    return wrapper\n</code></pre>"},{"location":"code/generator/#best-practices","title":"Best Practices","text":""},{"location":"code/generator/#prompt-design","title":"Prompt Design","text":"<ul> <li>Clear instructions</li> <li>Consistent format</li> <li>Citation requirements</li> <li>Error handling guidance</li> </ul>"},{"location":"code/generator/#api-usage","title":"API Usage","text":"<ul> <li>Proper error handling</li> <li>Rate limiting</li> <li>Retry logic</li> <li>Response validation</li> </ul>"},{"location":"code/generator/#output-quality","title":"Output Quality","text":"<ul> <li>Citation validation</li> <li>Response formatting</li> <li>Length control</li> <li>Safety checks</li> </ul>"},{"location":"code/generator/#example-usage","title":"Example Usage","text":"<pre><code># Initialize generator\ngenerator = AdvancedGenerator()\n\n# Generate answer\nquery = \"What is RAG?\"\ncontexts = [\n    \"RAG (Retrieval-Augmented Generation) is a technique...\",\n    \"This approach combines retrieval with generation...\"\n]\n\nresponse = generator.generate(\n    query=query,\n    contexts=contexts,\n    temperature=0.3,\n    citation_required=True\n)\n\nprint(response)\n</code></pre>"},{"location":"code/generator/#troubleshooting","title":"Troubleshooting","text":""},{"location":"code/generator/#api-errors","title":"API Errors","text":"<ul> <li>Check API key</li> <li>Verify rate limits</li> <li>Validate input format</li> </ul>"},{"location":"code/generator/#quality-issues","title":"Quality Issues","text":"<ul> <li>Adjust temperature</li> <li>Refine prompt</li> <li>Check context quality</li> </ul>"},{"location":"code/generator/#performance","title":"Performance","text":"<ul> <li>Cache responses</li> <li>Batch requests</li> <li>Monitor latency</li> </ul>"},{"location":"code/generator/#next-steps","title":"Next Steps","text":""},{"location":"code/generator/#further-reading","title":"Further Reading","text":"<ul> <li>End-to-End Pipeline</li> <li>Example Queries</li> <li>Evaluation Metrics</li> </ul>"},{"location":"code/pipeline/","title":"End-to-End Pipeline","text":"<p>This section explains how all components work together in the complete RAG pipeline, from document ingestion to answer generation.</p>"},{"location":"code/pipeline/#pipeline-overview","title":"Pipeline Overview","text":"<pre><code>graph TB\n    A[Document Input] --&gt; B[PDF Processing]\n    B --&gt; C[Text Chunking]\n    C --&gt; D[Embedding Generation]\n    D --&gt; E[Vector Store]\n\n    F[User Query] --&gt; G[Query Processing]\n    G --&gt; H[Retrieval]\n    E --&gt; H\n    H --&gt; I[Context Assembly]\n    I --&gt; J[LLM Generation]\n    J --&gt; K[Response]</code></pre>"},{"location":"code/pipeline/#complete-implementation","title":"Complete Implementation","text":""},{"location":"code/pipeline/#pipeline-class","title":"Pipeline Class","text":"<pre><code>class RAGPipeline:\n    def __init__(self, config=None):\n        self.config = config or self.default_config()\n        self.setup_components()\n\n    def setup_components(self):\n        \"\"\"Initialize all pipeline components.\"\"\"\n        # Initialize embedding model\n        self.embedding_model = SentenceTransformer(\n            self.config['embedding']['model_name']\n        )\n\n        # Initialize generator\n        self.generator = AdvancedGenerator(\n            model_name=self.config['generator']['model_name']\n        )\n\n        # Initialize empty index\n        self.index = None\n        self.chunks = []\n        self.metadata = {}\n\n    @staticmethod\n    def default_config():\n        return {\n            'embedding': {\n                'model_name': 'all-MiniLM-L6-v2',\n                'normalize': True\n            },\n            'chunking': {\n                'chunk_size': 1000,\n                'chunk_overlap': 200\n            },\n            'retriever': {\n                'top_k': 3,\n                'threshold': 0.6\n            },\n            'generator': {\n                'model_name': 'gemini-2.5-flash',\n                'temperature': 0.3\n            }\n        }\n\n    def ingest_document(self, file_path):\n        \"\"\"\n        Process a document and add to index.\n        \"\"\"\n        # Extract text\n        text = self.extract_text(file_path)\n\n        # Create chunks\n        new_chunks = self.create_chunks(\n            text,\n            size=self.config['chunking']['chunk_size'],\n            overlap=self.config['chunking']['chunk_overlap']\n        )\n\n        # Generate embeddings\n        embeddings = self.embedding_model.encode(\n            new_chunks,\n            normalize_embeddings=self.config['embedding']['normalize']\n        )\n\n        # Update index\n        self.update_index(new_chunks, embeddings)\n\n        return len(new_chunks)\n\n    def answer_question(self, query):\n        \"\"\"\n        Generate answer for a question.\n        \"\"\"\n        if not self.index:\n            raise ValueError(\"No documents indexed yet\")\n\n        # Get query embedding\n        query_emb = self.embedding_model.encode(\n            [query],\n            normalize_embeddings=self.config['embedding']['normalize']\n        )\n\n        # Retrieve relevant chunks\n        contexts = self.retrieve(\n            query_emb,\n            k=self.config['retriever']['top_k']\n        )\n\n        # Generate answer\n        response = self.generator.generate(\n            query=query,\n            contexts=[c['text'] for c in contexts],\n            temperature=self.config['generator']['temperature']\n        )\n\n        return {\n            'answer': response,\n            'sources': contexts\n        }\n</code></pre>"},{"location":"code/pipeline/#usage-example","title":"Usage Example","text":"<pre><code># Initialize pipeline\npipeline = RAGPipeline()\n\n# Ingest documents\ndoc_path = \"example.pdf\"\nnum_chunks = pipeline.ingest_document(doc_path)\nprint(f\"Processed {num_chunks} chunks\")\n\n# Ask questions\nquery = \"What are the key points about RAG?\"\nresult = pipeline.answer_question(query)\nprint(f\"Answer: {result['answer']}\")\nprint(\"\\nSources:\")\nfor src in result['sources']:\n    print(f\"- Score: {src['score']:.3f}\")\n    print(f\"  Text: {src['text'][:100]}...\")\n</code></pre>"},{"location":"code/pipeline/#pipeline-components","title":"Pipeline Components","text":""},{"location":"code/pipeline/#1-document-processing","title":"1. Document Processing","text":"<pre><code>def extract_text(self, file_path):\n    \"\"\"Extract text from PDF.\"\"\"\n    reader = PdfReader(file_path)\n    text = \"\"\n    for page in reader.pages:\n        text += page.extract_text()\n    return text\n\ndef create_chunks(self, text, size=1000, overlap=200):\n    \"\"\"Create overlapping text chunks.\"\"\"\n    chunks = []\n    start = 0\n\n    while start &lt; len(text):\n        end = start + size\n        chunk = text[start:end]\n        chunks.append(chunk)\n        start = end - overlap\n\n    return chunks\n</code></pre>"},{"location":"code/pipeline/#2-index-management","title":"2. Index Management","text":"<pre><code>def update_index(self, chunks, embeddings):\n    \"\"\"Update FAISS index with new embeddings.\"\"\"\n    # Convert to numpy array\n    embeddings = np.array(embeddings).astype('float32')\n\n    if self.index is None:\n        # Create new index\n        d = embeddings.shape[1]\n        self.index = faiss.IndexFlatL2(d)\n\n    # Add to index\n    self.index.add(embeddings)\n\n    # Update chunks list\n    start_idx = len(self.chunks)\n    self.chunks.extend(chunks)\n\n    # Add metadata\n    for i, chunk in enumerate(chunks, start=start_idx):\n        self.metadata[i] = {\n            'id': i,\n            'text': chunk\n        }\n</code></pre>"},{"location":"code/pipeline/#3-retrieval-logic","title":"3. Retrieval Logic","text":"<pre><code>def retrieve(self, query_emb, k=3):\n    \"\"\"Retrieve similar chunks.\"\"\"\n    # Search index\n    D, I = self.index.search(\n        query_emb.astype('float32'),\n        k\n    )\n\n    # Get results with metadata\n    results = []\n    for dist, idx in zip(D[0], I[0]):\n        results.append({\n            'text': self.chunks[idx],\n            'score': float(dist),\n            'metadata': self.metadata[idx]\n        })\n\n    return results\n</code></pre>"},{"location":"code/pipeline/#advanced-features","title":"Advanced Features","text":""},{"location":"code/pipeline/#1-batch-processing","title":"1. Batch Processing","text":"<pre><code>def batch_ingest_documents(self, file_paths, batch_size=32):\n    \"\"\"Process multiple documents in batches.\"\"\"\n    total_chunks = 0\n    for path in file_paths:\n        chunks = self.ingest_document(path)\n        total_chunks += chunks\n    return total_chunks\n</code></pre>"},{"location":"code/pipeline/#2-index-persistence","title":"2. Index Persistence","text":"<pre><code>def save_state(self, directory):\n    \"\"\"Save index and metadata.\"\"\"\n    os.makedirs(directory, exist_ok=True)\n\n    # Save FAISS index\n    index_path = os.path.join(directory, 'index.faiss')\n    faiss.write_index(self.index, index_path)\n\n    # Save metadata\n    meta_path = os.path.join(directory, 'metadata.json')\n    with open(meta_path, 'w') as f:\n        json.dump({\n            'chunks': self.chunks,\n            'metadata': self.metadata,\n            'config': self.config\n        }, f)\n\ndef load_state(self, directory):\n    \"\"\"Load saved index and metadata.\"\"\"\n    # Load FAISS index\n    index_path = os.path.join(directory, 'index.faiss')\n    self.index = faiss.read_index(index_path)\n\n    # Load metadata\n    meta_path = os.path.join(directory, 'metadata.json')\n    with open(meta_path, 'r') as f:\n        data = json.load(f)\n        self.chunks = data['chunks']\n        self.metadata = data['metadata']\n        self.config.update(data['config'])\n</code></pre>"},{"location":"code/pipeline/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling</li> <li>Validate inputs</li> <li>Graceful degradation</li> <li> <p>Clear error messages</p> </li> <li> <p>Performance</p> </li> <li>Batch processing</li> <li>Caching results</li> <li> <p>Index optimization</p> </li> <li> <p>Maintenance</p> </li> <li>Regular index updates</li> <li>State persistence</li> <li>Monitoring</li> </ol>"},{"location":"code/pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>See Examples</li> <li>Learn about Evaluation</li> <li>Explore Deployment</li> </ul>"},{"location":"code/retriever/","title":"Retriever Implementation","text":"<p>The retriever component is responsible for finding and returning the most relevant text chunks for a given query. This guide explains how our retrieval system works and how to optimize it.</p>"},{"location":"code/retriever/#overview","title":"Overview","text":"<p>The retriever: 1. Takes a user query 2. Converts it to an embedding 3. Finds similar document chunks 4. Returns the most relevant contexts</p>"},{"location":"code/retriever/#core-implementation","title":"Core Implementation","text":""},{"location":"code/retriever/#basic-retriever","title":"Basic Retriever","text":"<pre><code>def retrieve_context(query, embedding_model, index, text_chunks, k=3):\n    \"\"\"\n    Retrieve relevant context for a query.\n\n    Args:\n        query (str): User question\n        embedding_model: Initialized SentenceTransformer\n        index: FAISS index\n        text_chunks (list): Original text chunks\n        k (int): Number of chunks to retrieve\n\n    Returns:\n        list: Retrieved text chunks\n        list: Distance scores\n    \"\"\"\n    # Create query embedding\n    query_embedding = embedding_model.encode([query])\n\n    # Search the index\n    distances, indices = index.search(\n        np.array(query_embedding).astype('float32'),\n        k\n    )\n\n    # Get the text chunks\n    retrieved_chunks = [text_chunks[i] for i in indices[0]]\n\n    return retrieved_chunks, distances[0]\n</code></pre>"},{"location":"code/retriever/#advanced-retriever-with-metadata","title":"Advanced Retriever with Metadata","text":"<pre><code>class Retriever:\n    def __init__(self, embedding_model, index, chunks, metadata=None):\n        self.embedding_model = embedding_model\n        self.index = index\n        self.chunks = chunks\n        self.metadata = metadata or {}\n\n    def retrieve(self, query, k=3, threshold=0.6):\n        \"\"\"\n        Enhanced retrieval with metadata and filtering.\n        \"\"\"\n        # Get embeddings\n        query_emb = self.embedding_model.encode([query])\n\n        # Search\n        D, I = self.index.search(\n            np.array(query_emb).astype('float32'),\n            k\n        )\n\n        # Filter by threshold\n        mask = D[0] &lt; threshold\n        indices = I[0][mask]\n        scores = D[0][mask]\n\n        # Get chunks and metadata\n        results = []\n        for idx, score in zip(indices, scores):\n            results.append({\n                'text': self.chunks[idx],\n                'metadata': self.metadata.get(idx, {}),\n                'score': float(score)\n            })\n\n        return results\n</code></pre>"},{"location":"code/retriever/#reranking","title":"Reranking","text":"<p>For better precision, implement a reranking step:</p> <pre><code>from sentence_transformers import CrossEncoder\n\nclass RerankedRetriever(Retriever):\n    def __init__(self, *args, reranker_model='cross-encoder/ms-marco-MiniLM-L-6-v2'):\n        super().__init__(*args)\n        self.reranker = CrossEncoder(reranker_model)\n\n    def retrieve(self, query, k=10, final_k=3):\n        # Get initial candidates\n        candidates = super().retrieve(query, k=k)\n\n        # Rerank\n        pairs = [[query, c['text']] for c in candidates]\n        scores = self.reranker.predict(pairs)\n\n        # Sort and filter\n        ranked = sorted(zip(candidates, scores), \n                       key=lambda x: x[1], \n                       reverse=True)\n\n        return [r[0] for r in ranked[:final_k]]\n</code></pre>"},{"location":"code/retriever/#hybrid-retrieval","title":"Hybrid Retrieval","text":"<p>Combine dense and sparse retrieval:</p> <pre><code>from rank_bm25 import BM25Okapi\nimport numpy as np\n\nclass HybridRetriever:\n    def __init__(self, dense_retriever, chunks):\n        self.dense = dense_retriever\n        # Initialize BM25\n        tokenized_chunks = [chunk.split() for chunk in chunks]\n        self.bm25 = BM25Okapi(tokenized_chunks)\n        self.chunks = chunks\n\n    def retrieve(self, query, k=3, alpha=0.5):\n        # Dense scores\n        dense_results = self.dense.retrieve(query, k=k)\n        dense_scores = {r['text']: r['score'] \n                       for r in dense_results}\n\n        # Sparse scores\n        sparse_scores = self.bm25.get_scores(query.split())\n        norm_sparse = (sparse_scores - np.min(sparse_scores)) / \\\n                     (np.max(sparse_scores) - np.min(sparse_scores))\n\n        # Combine scores\n        final_scores = {}\n        for i, chunk in enumerate(self.chunks):\n            if chunk in dense_scores:\n                final_scores[chunk] = alpha * dense_scores[chunk] + \\\n                                    (1-alpha) * norm_sparse[i]\n\n        # Sort and return top k\n        ranked = sorted(final_scores.items(), \n                       key=lambda x: x[1], \n                       reverse=True)\n\n        return [{'text': r[0], 'score': r[1]} \n                for r in ranked[:k]]\n</code></pre>"},{"location":"code/retriever/#configuration","title":"Configuration","text":"<p>Retriever parameters in <code>config.yaml</code>:</p> <pre><code>retriever:\n  initial_k: 10\n  final_k: 3\n  similarity_threshold: 0.6\n  hybrid:\n    enabled: true\n    alpha: 0.5\n  reranking:\n    enabled: true\n    model: cross-encoder/ms-marco-MiniLM-L-6-v2\n</code></pre>"},{"location":"code/retriever/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Parameter Tuning</p> <ul> <li>Start with larger <code>k</code> and filter down</li> <li>Adjust threshold based on corpus</li> <li>Monitor retrieval quality</li> </ul> </li> <li> <p>Performance</p> <ul> <li>Cache query embeddings</li> <li>Batch similar queries</li> <li>Use approximate search for scale</li> </ul> </li> <li> <p>Quality</p> <ul> <li>Implement reranking for precision</li> <li>Use hybrid retrieval for robustness</li> <li>Track retrieval metrics</li> </ul> </li> </ol>"},{"location":"code/retriever/#evaluation","title":"Evaluation","text":""},{"location":"code/retriever/#retrieval-metrics","title":"Retrieval Metrics","text":"<pre><code>def evaluate_retrieval(retriever, queries, ground_truth):\n    \"\"\"\n    Evaluate retriever performance.\n\n    Args:\n        retriever: Retriever instance\n        queries: List of test queries\n        ground_truth: Dict mapping queries to relevant chunks\n\n    Returns:\n        dict: Metrics including recall, precision, MRR\n    \"\"\"\n    metrics = {\n        'recall': [],\n        'precision': [],\n        'mrr': []\n    }\n\n    for query in queries:\n        results = retriever.retrieve(query)\n        retrieved = {r['text'] for r in results}\n        relevant = ground_truth[query]\n\n        # Calculate metrics\n        recall = len(retrieved &amp; relevant) / len(relevant)\n        precision = len(retrieved &amp; relevant) / len(retrieved)\n\n        # MRR calculation\n        ranks = []\n        for i, r in enumerate(results, 1):\n            if r['text'] in relevant:\n                ranks.append(1/i)\n        mrr = max(ranks) if ranks else 0\n\n        metrics['recall'].append(recall)\n        metrics['precision'].append(precision)\n        metrics['mrr'].append(mrr)\n\n    return {k: np.mean(v) for k, v in metrics.items()}\n</code></pre>"},{"location":"code/retriever/#debugging","title":"Debugging","text":"<p>Common issues and solutions:</p> <ol> <li>Poor Retrieval Quality</li> <li>Check embedding quality</li> <li>Validate chunk sizes</li> <li> <p>Adjust similarity threshold</p> </li> <li> <p>Slow Retrieval</p> </li> <li>Profile index search time</li> <li>Consider approximate search</li> <li> <p>Optimize batch size</p> </li> <li> <p>Out of Memory</p> </li> <li>Reduce initial k</li> <li>Use memory-efficient index</li> <li>Stream results if needed</li> </ol>"},{"location":"code/retriever/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Generator Integration</li> <li>Explore the End-to-End Pipeline</li> <li>See Example Queries</li> </ul>"},{"location":"code/vector-store/","title":"Vector Store","text":"<p>This section explains how we use FAISS (Facebook AI Similarity Search) to store and retrieve embeddings efficiently.</p>"},{"location":"code/vector-store/#overview","title":"Overview","text":"<p>FAISS is a library for efficient similarity search of dense vectors. We use it to: 1. Store document embeddings 2. Perform fast nearest neighbor search 3. Scale to large document collections</p>"},{"location":"code/vector-store/#implementation","title":"Implementation","text":""},{"location":"code/vector-store/#setting-up-faiss","title":"Setting up FAISS","text":"<pre><code>import faiss\nimport numpy as np\n\ndef create_faiss_index(dimension, index_type='L2'):\n    \"\"\"\n    Create a FAISS index.\n\n    Args:\n        dimension (int): Embedding dimension\n        index_type (str): Type of index ('L2' or 'IP')\n\n    Returns:\n        faiss.Index: Initialized FAISS index\n    \"\"\"\n    if index_type == 'L2':\n        return faiss.IndexFlatL2(dimension)\n    elif index_type == 'IP':\n        return faiss.IndexFlatIP(dimension)\n    else:\n        raise ValueError(\"Invalid index type\")\n</code></pre>"},{"location":"code/vector-store/#adding-vectors","title":"Adding Vectors","text":"<pre><code>def add_to_index(index, embeddings):\n    \"\"\"\n    Add embeddings to the FAISS index.\n\n    Args:\n        index (faiss.Index): FAISS index\n        embeddings (np.ndarray): Matrix of embeddings\n\n    Returns:\n        int: Number of vectors added\n    \"\"\"\n    # Convert to float32 (required by FAISS)\n    embeddings = np.array(embeddings).astype('float32')\n\n    # Add to index\n    index.add(embeddings)\n\n    return index.ntotal\n</code></pre>"},{"location":"code/vector-store/#similarity-search","title":"Similarity Search","text":"<pre><code>def search_index(index, query_embedding, k=5):\n    \"\"\"\n    Search for similar vectors.\n\n    Args:\n        index (faiss.Index): FAISS index\n        query_embedding (np.ndarray): Query vector\n        k (int): Number of results to return\n\n    Returns:\n        tuple: (distances, indices)\n    \"\"\"\n    # Ensure query is float32 and 2D\n    query_embedding = np.array(query_embedding).astype('float32')\n    if query_embedding.ndim == 1:\n        query_embedding = query_embedding.reshape(1, -1)\n\n    # Perform search\n    distances, indices = index.search(query_embedding, k)\n\n    return distances[0], indices[0]\n</code></pre>"},{"location":"code/vector-store/#advanced-index-types","title":"Advanced Index Types","text":""},{"location":"code/vector-store/#ivf-index-for-large-datasets","title":"IVF Index (for Large Datasets)","text":"<pre><code>def create_ivf_index(dimension, nlist=100):\n    \"\"\"\n    Create an IVF index for faster search.\n\n    Args:\n        dimension (int): Embedding dimension\n        nlist (int): Number of clusters\n\n    Returns:\n        faiss.Index: IVF index\n    \"\"\"\n    quantizer = faiss.IndexFlatL2(dimension)\n    index = faiss.IndexIVFFlat(\n        quantizer, \n        dimension,\n        nlist,\n        faiss.METRIC_L2\n    )\n\n    return index\n</code></pre>"},{"location":"code/vector-store/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code>def use_gpu_index(index):\n    \"\"\"\n    Move index to GPU.\n\n    Args:\n        index (faiss.Index): CPU index\n\n    Returns:\n        faiss.Index: GPU index\n    \"\"\"\n    res = faiss.StandardGpuResources()\n    return faiss.index_cpu_to_gpu(res, 0, index)\n</code></pre>"},{"location":"code/vector-store/#complete-example","title":"Complete Example","text":"<pre><code>def setup_vector_store(embeddings, dimension=384):\n    \"\"\"\n    Set up a complete vector store.\n\n    Args:\n        embeddings (np.ndarray): Initial embeddings\n        dimension (int): Embedding dimension\n\n    Returns:\n        tuple: (index, total_vectors)\n    \"\"\"\n    # Create index\n    index = create_faiss_index(dimension)\n\n    # Add vectors\n    total = add_to_index(index, embeddings)\n\n    print(f\"Added {total} vectors to index\")\n\n    return index, total\n\n# Example usage\ndimension = 384  # for all-MiniLM-L6-v2\nembeddings = np.random.random((100, dimension)).astype('float32')\n\n# Setup store\nindex, total = setup_vector_store(embeddings)\n\n# Search example\nquery = np.random.random(dimension).astype('float32')\ndistances, indices = search_index(index, query, k=5)\n\nprint(\"Search results:\")\nfor d, i in zip(distances, indices):\n    print(f\"Index: {i}, Distance: {d:.4f}\")\n</code></pre>"},{"location":"code/vector-store/#performance-optimization","title":"Performance Optimization","text":""},{"location":"code/vector-store/#1-index-types","title":"1. Index Types","text":"<p>Choose based on your needs: - Flat: Best accuracy, slower - IVF: Faster search, slight accuracy loss - HNSW: Very fast, memory intensive</p>"},{"location":"code/vector-store/#2-gpu-usage","title":"2. GPU Usage","text":"<p>When to use GPU: - Large datasets (&gt;1M vectors) - Need for real-time search - GPU memory available</p>"},{"location":"code/vector-store/#3-batch-processing","title":"3. Batch Processing","text":"<p>For multiple queries:</p> <pre><code>def batch_search(index, queries, k=5):\n    \"\"\"\n    Search for multiple queries at once.\n\n    Args:\n        index (faiss.Index): FAISS index\n        queries (np.ndarray): Query vectors\n        k (int): Results per query\n\n    Returns:\n        tuple: (distances, indices)\n    \"\"\"\n    queries = np.array(queries).astype('float32')\n    return index.search(queries, k)\n</code></pre>"},{"location":"code/vector-store/#storage-and-persistence","title":"Storage and Persistence","text":""},{"location":"code/vector-store/#save-index","title":"Save Index","text":"<pre><code>def save_index(index, filepath):\n    \"\"\"\n    Save FAISS index to disk.\n\n    Args:\n        index (faiss.Index): FAISS index\n        filepath (str): Output path\n    \"\"\"\n    faiss.write_index(index, filepath)\n</code></pre>"},{"location":"code/vector-store/#load-index","title":"Load Index","text":"<pre><code>def load_index(filepath):\n    \"\"\"\n    Load FAISS index from disk.\n\n    Args:\n        filepath (str): Path to saved index\n\n    Returns:\n        faiss.Index: Loaded index\n    \"\"\"\n    return faiss.read_index(filepath)\n</code></pre>"},{"location":"code/vector-store/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Index Selection</p> <ul> <li>Use Flat index for &lt;1M vectors</li> <li>Use IVF for &gt;1M vectors</li> <li>Consider HNSW for speed priority</li> </ul> </li> <li> <p>Resource Management</p> <ul> <li>Monitor memory usage</li> <li>Use GPU strategically</li> <li>Implement batch processing</li> </ul> </li> <li> <p>Maintenance</p> <ul> <li>Regular index saves</li> <li>Monitoring index size</li> <li>Performance testing</li> </ul> </li> </ol>"},{"location":"code/vector-store/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Retrieval</li> <li>Explore End-to-End Pipeline</li> <li>See Examples</li> </ul>"},{"location":"examples/basic-retrieval/","title":"Basic Text Retrieval Example","text":"<p>This guide walks through a complete example of setting up and using the RAG system for basic text retrieval and question answering.</p>"},{"location":"examples/basic-retrieval/#setup","title":"Setup","text":"<p>First, let's set up the basic environment and import required packages:</p> <pre><code>import os\nimport google.generativeai as genai\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\nfrom pypdf import PdfReader\n\n# Configure API\nos.environ[\"GEMINI_API_KEY\"] = \"your-api-key-here\"\ngenai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n</code></pre>"},{"location":"examples/basic-retrieval/#sample-document","title":"Sample Document","text":"<p>Let's create a simple example document:</p> <pre><code>sample_text = \"\"\"\nRetrieval-Augmented Generation (RAG) is a technique that enhances language models\nby providing them with relevant external information during generation. This approach\ncombines the benefits of retrieval-based and generation-based methods.\n\nRAG works in three main steps:\n1. When a user asks a question, the system searches a knowledge base to find relevant information\n2. The retrieved information is combined with the original question\n3. This enhanced prompt is sent to a language model to generate an answer\n\nKey benefits of RAG include:\n- Improved accuracy by grounding responses in specific documents\n- Reduced hallucination by providing concrete context\n- Ability to answer questions about new or updated information\n- More transparent and verifiable responses\n\"\"\"\n\n# Split into chunks\nchunks = [sample_text[i:i + 200] for i in range(0, len(sample_text), 200)]\n</code></pre>"},{"location":"examples/basic-retrieval/#creating-embeddings","title":"Creating Embeddings","text":"<p>Initialize the embedding model and create embeddings:</p> <pre><code># Initialize embedding model\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings\nembeddings = embedding_model.encode(chunks)\nprint(f\"Created {len(embeddings)} embeddings of dimension {embeddings.shape[1]}\")\n</code></pre>"},{"location":"examples/basic-retrieval/#setting-up-faiss","title":"Setting up FAISS","text":"<p>Create and populate the vector store:</p> <pre><code># Initialize FAISS index\ndimension = embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)\n\n# Add vectors to index\nindex.add(np.array(embeddings).astype('float32'))\nprint(f\"Added {index.ntotal} vectors to index\")\n</code></pre>"},{"location":"examples/basic-retrieval/#basic-retrieval-function","title":"Basic Retrieval Function","text":"<pre><code>def retrieve_context(query, k=2):\n    \"\"\"\n    Retrieve relevant chunks for a query.\n\n    Args:\n        query (str): User question\n        k (int): Number of chunks to retrieve\n\n    Returns:\n        list: Retrieved text chunks\n        list: Similarity scores\n    \"\"\"\n    # Create query embedding\n    query_emb = embedding_model.encode([query])\n\n    # Search index\n    distances, indices = index.search(\n        np.array(query_emb).astype('float32'),\n        k\n    )\n\n    # Get chunks and scores\n    results = []\n    for idx, score in zip(indices[0], distances[0]):\n        results.append({\n            'text': chunks[idx],\n            'score': float(score)\n        })\n\n    return results\n</code></pre>"},{"location":"examples/basic-retrieval/#question-answering","title":"Question Answering","text":"<p>Let's implement a simple question-answering function:</p> <pre><code>def answer_question(query):\n    \"\"\"\n    Answer a question using RAG.\n    \"\"\"\n    # Get relevant chunks\n    results = retrieve_context(query, k=2)\n\n    # Build context\n    context = \"\\n\\n\".join(r['text'] for r in results)\n\n    # Create prompt\n    prompt = f\"\"\"Based on the following context, answer the question.\n    If you cannot find relevant information, say so.\n\n    Context:\n    {context}\n\n    Question: {query}\n\n    Answer:\"\"\"\n\n    # Generate answer\n    model = genai.GenerativeModel('gemini-2.5-flash')\n    response = model.generate_content(prompt)\n\n    return {\n        'answer': response.text,\n        'sources': results\n    }\n</code></pre>"},{"location":"examples/basic-retrieval/#example-usage","title":"Example Usage","text":"<p>Let's try some example questions:</p> <pre><code># Example 1: Basic factual question\nquestion1 = \"What is RAG and how does it work?\"\nresult1 = answer_question(question1)\nprint(\"Question:\", question1)\nprint(\"Answer:\", result1['answer'])\nprint(\"\\nSources:\")\nfor src in result1['sources']:\n    print(f\"Score: {src['score']:.3f}\")\n    print(f\"Text: {src['text'][:100]}...\")\nprint(\"\\n\")\n\n# Example 2: Specific detail question\nquestion2 = \"What are the benefits of using RAG?\"\nresult2 = answer_question(question2)\nprint(\"Question:\", question2)\nprint(\"Answer:\", result2['answer'])\nprint(\"\\nSources:\")\nfor src in result2['sources']:\n    print(f\"Score: {src['score']:.3f}\")\n    print(f\"Text: {src['text'][:100]}...\")\n</code></pre>"},{"location":"examples/basic-retrieval/#expected-output","title":"Expected Output","text":"<p>For the first question, you might see:</p> <pre><code>Question: What is RAG and how does it work?\nAnswer: RAG (Retrieval-Augmented Generation) is a technique that enhances language models by providing them with external information during generation. It works in three main steps:\n\n1. When a user asks a question, the system searches a knowledge base for relevant information\n2. The retrieved information is combined with the original question\n3. This enhanced prompt is sent to a language model to generate an answer\n\nSources:\nScore: 0.235\nText: Retrieval-Augmented Generation (RAG) is a technique that enhances language models by providing them with relev...\nScore: 0.412\nText: RAG works in three main steps: 1. When a user asks a question, the system searches a knowledge base to find...\n</code></pre>"},{"location":"examples/basic-retrieval/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/basic-retrieval/#1-filtering-by-score","title":"1. Filtering by Score","text":"<pre><code>def retrieve_with_threshold(query, threshold=0.5):\n    \"\"\"Retrieve chunks with score filtering.\"\"\"\n    results = retrieve_context(query, k=5)\n    return [r for r in results if r['score'] &lt; threshold]\n</code></pre>"},{"location":"examples/basic-retrieval/#2-contextual-history","title":"2. Contextual History","text":"<pre><code>class RAGChat:\n    def __init__(self):\n        self.history = []\n\n    def chat(self, query):\n        # Get answer\n        result = answer_question(query)\n\n        # Update history\n        self.history.append({\n            'query': query,\n            'response': result\n        })\n\n        return result\n\n    def get_chat_history(self):\n        return self.history\n</code></pre>"},{"location":"examples/basic-retrieval/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ol> <li>Poor Retrieval Results</li> <li>Try increasing <code>k</code></li> <li>Adjust chunk size</li> <li> <p>Check embedding quality</p> </li> <li> <p>Irrelevant Answers</p> </li> <li>Review context window</li> <li>Adjust prompt template</li> <li> <p>Lower temperature</p> </li> <li> <p>Performance Issues</p> </li> <li>Batch embeddings</li> <li>Optimize index</li> <li>Cache results</li> </ol>"},{"location":"examples/basic-retrieval/#next-steps","title":"Next Steps","text":"<ul> <li>Try PDF Document Processing</li> <li>Build a Custom Knowledge Base</li> <li>Learn about Evaluation</li> </ul>"},{"location":"examples/evaluation/","title":"Evaluation and Metrics","text":"<p>This guide explains how to evaluate your RAG system's performance using various metrics and techniques.</p>"},{"location":"examples/evaluation/#overview","title":"Overview","text":"<p>We'll cover: 1. Retrieval evaluation 2. Answer quality assessment 3. System performance metrics 4. End-to-end testing</p>"},{"location":"examples/evaluation/#implementation","title":"Implementation","text":""},{"location":"examples/evaluation/#1-retrieval-evaluation","title":"1. Retrieval Evaluation","text":"<pre><code>import numpy as np\nfrom sklearn.metrics import precision_recall_curve\nfrom typing import List, Dict\n\ndef evaluate_retrieval(retriever, test_queries: List[Dict]):\n    \"\"\"\n    Evaluate retrieval performance.\n\n    Args:\n        retriever: Retriever instance\n        test_queries: List of dicts with 'query' and 'relevant_docs'\n\n    Returns:\n        dict: Metrics including recall, precision, MRR\n    \"\"\"\n    metrics = {\n        'recall@k': [],\n        'precision@k': [],\n        'mrr': [],\n        'ndcg': []\n    }\n\n    for query in test_queries:\n        # Get retrieval results\n        results = retriever.retrieve(\n            query['query'],\n            k=10\n        )\n\n        # Get retrieved doc ids\n        retrieved_ids = [r['metadata']['id'] \n                        for r in results]\n\n        # Calculate metrics\n        metrics['recall@k'].append(\n            calculate_recall_at_k(\n                retrieved_ids,\n                query['relevant_docs']\n            )\n        )\n\n        metrics['precision@k'].append(\n            calculate_precision_at_k(\n                retrieved_ids,\n                query['relevant_docs']\n            )\n        )\n\n        metrics['mrr'].append(\n            calculate_mrr(\n                retrieved_ids,\n                query['relevant_docs']\n            )\n        )\n\n        metrics['ndcg'].append(\n            calculate_ndcg(\n                retrieved_ids,\n                query['relevant_docs']\n            )\n        )\n\n    # Average metrics\n    return {k: np.mean(v) for k, v in metrics.items()}\n</code></pre>"},{"location":"examples/evaluation/#2-answer-quality-metrics","title":"2. Answer Quality Metrics","text":"<pre><code>from rouge_score import rouge_scorer\nfrom bert_score import score\n\ndef evaluate_answer_quality(\n    generated_answers: List[str],\n    reference_answers: List[str]\n):\n    \"\"\"\n    Evaluate answer quality using various metrics.\n    \"\"\"\n    # Initialize ROUGE scorer\n    scorer = rouge_scorer.RougeScorer(\n        ['rouge1', 'rouge2', 'rougeL'],\n        use_stemmer=True\n    )\n\n    metrics = {\n        'rouge': {},\n        'bert_score': {},\n        'answer_length': []\n    }\n\n    # Calculate ROUGE scores\n    for gen, ref in zip(generated_answers, reference_answers):\n        scores = scorer.score(gen, ref)\n\n        for key, value in scores.items():\n            if key not in metrics['rouge']:\n                metrics['rouge'][key] = []\n            metrics['rouge'][key].append(value.fmeasure)\n\n        # Calculate BERTScore\n        P, R, F1 = score(\n            [gen],\n            [ref],\n            lang='en',\n            verbose=False\n        )\n        metrics['bert_score']['precision'] = P.mean().item()\n        metrics['bert_score']['recall'] = R.mean().item()\n        metrics['bert_score']['f1'] = F1.mean().item()\n\n        # Answer length\n        metrics['answer_length'].append(len(gen.split()))\n\n    # Average metrics\n    for key in metrics['rouge']:\n        metrics['rouge'][key] = np.mean(metrics['rouge'][key])\n\n    metrics['answer_length'] = np.mean(metrics['answer_length'])\n\n    return metrics\n</code></pre>"},{"location":"examples/evaluation/#3-performance-metrics","title":"3. Performance Metrics","text":"<pre><code>import time\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass PerformanceMetrics:\n    retrieval_time: float\n    generation_time: float\n    total_time: float\n    chunks_retrieved: int\n    tokens_generated: Optional[int] = None\n\nclass PerformanceMonitor:\n    def __init__(self):\n        self.metrics = []\n\n    def measure_pipeline(self, pipeline, query):\n        \"\"\"Measure pipeline performance.\"\"\"\n        start_time = time.time()\n\n        # Measure retrieval\n        retrieval_start = time.time()\n        contexts = pipeline.retrieve(query)\n        retrieval_time = time.time() - retrieval_start\n\n        # Measure generation\n        generation_start = time.time()\n        response = pipeline.generate(query, contexts)\n        generation_time = time.time() - generation_start\n\n        # Calculate metrics\n        total_time = time.time() - start_time\n\n        metrics = PerformanceMetrics(\n            retrieval_time=retrieval_time,\n            generation_time=generation_time,\n            total_time=total_time,\n            chunks_retrieved=len(contexts)\n        )\n\n        self.metrics.append(metrics)\n        return metrics\n\n    def get_summary(self):\n        \"\"\"Get performance summary.\"\"\"\n        if not self.metrics:\n            return {}\n\n        return {\n            'avg_retrieval_time': np.mean(\n                [m.retrieval_time for m in self.metrics]\n            ),\n            'avg_generation_time': np.mean(\n                [m.generation_time for m in self.metrics]\n            ),\n            'avg_total_time': np.mean(\n                [m.total_time for m in self.metrics]\n            ),\n            'avg_chunks': np.mean(\n                [m.chunks_retrieved for m in self.metrics]\n            )\n        }\n</code></pre>"},{"location":"examples/evaluation/#4-end-to-end-testing","title":"4. End-to-End Testing","text":"<pre><code>def run_e2e_evaluation(pipeline, test_cases):\n    \"\"\"\n    Run end-to-end evaluation.\n\n    Args:\n        pipeline: RAG pipeline instance\n        test_cases: List of test cases\n\n    Returns:\n        dict: Evaluation results\n    \"\"\"\n    results = {\n        'retrieval': [],\n        'answer_quality': [],\n        'performance': []\n    }\n\n    monitor = PerformanceMonitor()\n\n    for test in test_cases:\n        # Measure performance\n        metrics = monitor.measure_pipeline(\n            pipeline,\n            test['query']\n        )\n\n        # Get pipeline response\n        response = pipeline.answer_question(\n            test['query']\n        )\n\n        # Evaluate retrieval\n        retrieval_metrics = evaluate_retrieval(\n            pipeline.retriever,\n            [test]\n        )\n\n        # Evaluate answer quality\n        quality_metrics = evaluate_answer_quality(\n            [response['answer']],\n            [test['reference_answer']]\n        )\n\n        # Store results\n        results['retrieval'].append(retrieval_metrics)\n        results['answer_quality'].append(quality_metrics)\n        results['performance'].append(metrics)\n\n    # Aggregate results\n    summary = {\n        'retrieval': aggregate_metrics(results['retrieval']),\n        'answer_quality': aggregate_metrics(\n            results['answer_quality']\n        ),\n        'performance': monitor.get_summary()\n    }\n\n    return summary, results\n</code></pre>"},{"location":"examples/evaluation/#example-usage","title":"Example Usage","text":""},{"location":"examples/evaluation/#1-create-test-cases","title":"1. Create Test Cases","text":"<pre><code>test_cases = [\n    {\n        'query': 'What is RAG?',\n        'relevant_docs': ['doc1', 'doc2'],\n        'reference_answer': 'RAG is a technique that...'\n    },\n    {\n        'query': 'How does RAG work?',\n        'relevant_docs': ['doc2', 'doc3'],\n        'reference_answer': 'RAG works by retrieving...'\n    }\n]\n</code></pre>"},{"location":"examples/evaluation/#2-run-evaluation","title":"2. Run Evaluation","text":"<pre><code># Initialize pipeline\npipeline = RAGPipeline()\n\n# Run evaluation\nsummary, detailed = run_e2e_evaluation(\n    pipeline,\n    test_cases\n)\n\n# Print results\nprint(\"Evaluation Summary:\")\nprint(\"\\nRetrieval Metrics:\")\nfor k, v in summary['retrieval'].items():\n    print(f\"{k}: {v:.3f}\")\n\nprint(\"\\nAnswer Quality:\")\nfor k, v in summary['answer_quality'].items():\n    print(f\"{k}: {v:.3f}\")\n\nprint(\"\\nPerformance:\")\nfor k, v in summary['performance'].items():\n    print(f\"{k}: {v:.3f}s\")\n</code></pre>"},{"location":"examples/evaluation/#visualization","title":"Visualization","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_metrics(results):\n    \"\"\"Plot evaluation metrics.\"\"\"\n    # Set up the figure\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n    # Retrieval metrics\n    retrieval_df = pd.DataFrame(results['retrieval'])\n    sns.boxplot(data=retrieval_df, ax=axes[0,0])\n    axes[0,0].set_title('Retrieval Metrics')\n\n    # Answer quality\n    quality_df = pd.DataFrame(results['answer_quality'])\n    sns.boxplot(data=quality_df, ax=axes[0,1])\n    axes[0,1].set_title('Answer Quality')\n\n    # Performance\n    perf_df = pd.DataFrame(\n        [vars(m) for m in results['performance']]\n    )\n    sns.boxplot(data=perf_df, ax=axes[1,0])\n    axes[1,0].set_title('Performance Metrics')\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"examples/evaluation/#best-practices","title":"Best Practices","text":"<ol> <li>Test Data Preparation</li> <li>Create diverse test cases</li> <li>Include edge cases</li> <li> <p>Maintain gold standard answers</p> </li> <li> <p>Evaluation Strategy</p> </li> <li>Regular benchmarking</li> <li>A/B testing new features</li> <li> <p>Monitoring drift</p> </li> <li> <p>Metric Selection</p> </li> <li>Choose task-appropriate metrics</li> <li>Consider human evaluation</li> <li>Track trends over time</li> </ol>"},{"location":"examples/evaluation/#next-steps","title":"Next Steps","text":"<ul> <li>Implement Custom Knowledge Base</li> <li>Explore Deployment Options</li> <li>Setup Monitoring</li> </ul>"},{"location":"examples/pdf-processing/","title":"PDF Document Processing","text":"<p>This guide demonstrates how to process PDF documents effectively in your RAG system.</p>"},{"location":"examples/pdf-processing/#overview","title":"Overview","text":"<p>Processing PDFs involves: 1. Text extraction 2. Layout analysis 3. Content cleaning 4. Intelligent chunking 5. Metadata preservation</p>"},{"location":"examples/pdf-processing/#implementation","title":"Implementation","text":""},{"location":"examples/pdf-processing/#1-enhanced-pdf-processing","title":"1. Enhanced PDF Processing","text":"<pre><code>from pypdf import PdfReader\nimport re\nfrom typing import List, Dict\n\nclass PDFProcessor:\n    def __init__(self, config=None):\n        self.config = config or self.default_config()\n\n    @staticmethod\n    def default_config():\n        return {\n            'chunk_size': 1000,\n            'chunk_overlap': 200,\n            'min_chunk_size': 100,\n            'clean_text': True,\n            'preserve_layout': True\n        }\n\n    def process_pdf(self, file_path: str) -&gt; List[Dict]:\n        \"\"\"\n        Process PDF and return chunks with metadata.\n\n        Args:\n            file_path: Path to PDF file\n\n        Returns:\n            list: List of dicts with text and metadata\n        \"\"\"\n        # Read PDF\n        reader = PdfReader(file_path)\n\n        # Extract text and metadata\n        chunks = []\n        for page_num, page in enumerate(reader.pages, 1):\n            # Get page text\n            text = page.extract_text()\n\n            # Clean text if enabled\n            if self.config['clean_text']:\n                text = self.clean_text(text)\n\n            # Create chunks\n            page_chunks = self.create_chunks(\n                text,\n                self.config['chunk_size'],\n                self.config['chunk_overlap']\n            )\n\n            # Add metadata\n            for i, chunk in enumerate(page_chunks):\n                chunks.append({\n                    'text': chunk,\n                    'metadata': {\n                        'page': page_num,\n                        'chunk_num': i + 1,\n                        'source': file_path\n                    }\n                })\n\n        return chunks\n\n    def clean_text(self, text: str) -&gt; str:\n        \"\"\"Clean and normalize text.\"\"\"\n        # Remove excessive whitespace\n        text = re.sub(r'\\s+', ' ', text)\n\n        # Fix common PDF artifacts\n        text = re.sub(r'([a-z])-\\s+([a-z])', r'\\1\\2', text)\n\n        # Remove header/footer artifacts\n        text = re.sub(r'\\d+\\s+of\\s+\\d+', '', text)\n\n        return text.strip()\n\n    def create_chunks(\n        self,\n        text: str,\n        chunk_size: int,\n        overlap: int\n    ) -&gt; List[str]:\n        \"\"\"Create overlapping chunks preserving structure.\"\"\"\n        chunks = []\n        start = 0\n\n        while start &lt; len(text):\n            # Find end of chunk\n            end = start + chunk_size\n\n            if end &lt; len(text):\n                # Try to end at sentence boundary\n                next_period = text.find('.', end - 50, end + 50)\n                if next_period != -1:\n                    end = next_period + 1\n\n            # Get chunk\n            chunk = text[start:end].strip()\n\n            # Only add if meets minimum size\n            if len(chunk) &gt;= self.config['min_chunk_size']:\n                chunks.append(chunk)\n\n            # Move start position\n            start = end - overlap\n\n        return chunks\n</code></pre>"},{"location":"examples/pdf-processing/#2-layout-aware-processing","title":"2. Layout-Aware Processing","text":"<pre><code>from pdfminer.high_level import extract_pages\nfrom pdfminer.layout import LTTextContainer\n\nclass LayoutAwarePDFProcessor(PDFProcessor):\n    def process_pdf(self, file_path: str) -&gt; List[Dict]:\n        \"\"\"Process PDF preserving layout elements.\"\"\"\n        chunks = []\n\n        # Extract pages with layout\n        for page_num, page_layout in enumerate(\n            extract_pages(file_path), 1\n        ):\n            # Extract text elements\n            elements = []\n            for element in page_layout:\n                if isinstance(element, LTTextContainer):\n                    elements.append({\n                        'text': element.get_text(),\n                        'bbox': element.bbox,\n                        'type': type(element).__name__\n                    })\n\n            # Sort by position (top to bottom, left to right)\n            elements.sort(\n                key=lambda e: (-e['bbox'][1], e['bbox'][0])\n            )\n\n            # Combine elements into coherent text\n            page_text = self.combine_elements(elements)\n\n            # Create chunks\n            page_chunks = self.create_chunks(\n                page_text,\n                self.config['chunk_size'],\n                self.config['chunk_overlap']\n            )\n\n            # Add metadata\n            for i, chunk in enumerate(page_chunks):\n                chunks.append({\n                    'text': chunk,\n                    'metadata': {\n                        'page': page_num,\n                        'chunk_num': i + 1,\n                        'source': file_path,\n                        'layout_preserved': True\n                    }\n                })\n\n        return chunks\n\n    def combine_elements(self, elements: List[Dict]) -&gt; str:\n        \"\"\"Combine text elements respecting layout.\"\"\"\n        text = \"\"\n        current_line_y = None\n\n        for elem in elements:\n            # Check if new line needed\n            if current_line_y is not None:\n                if abs(elem['bbox'][1] - current_line_y) &gt; 5:\n                    text += \"\\n\"\n\n            # Add text\n            text += elem['text'].strip() + \" \"\n            current_line_y = elem['bbox'][1]\n\n        return text.strip()\n</code></pre>"},{"location":"examples/pdf-processing/#usage-example","title":"Usage Example","text":""},{"location":"examples/pdf-processing/#1-basic-processing","title":"1. Basic Processing","text":"<pre><code># Initialize processor\nprocessor = PDFProcessor({\n    'chunk_size': 1000,\n    'chunk_overlap': 200,\n    'clean_text': True\n})\n\n# Process PDF\nfile_path = \"example.pdf\"\nchunks = processor.process_pdf(file_path)\n\nprint(f\"Processed {len(chunks)} chunks\")\n\n# Preview chunks\nfor i, chunk in enumerate(chunks[:3]):\n    print(f\"\\nChunk {i+1}:\")\n    print(f\"Page: {chunk['metadata']['page']}\")\n    print(f\"Text: {chunk['text'][:100]}...\")\n</code></pre>"},{"location":"examples/pdf-processing/#2-layout-aware-processing_1","title":"2. Layout-Aware Processing","text":"<pre><code># Initialize layout-aware processor\nprocessor = LayoutAwarePDFProcessor({\n    'chunk_size': 1000,\n    'chunk_overlap': 200,\n    'preserve_layout': True\n})\n\n# Process PDF\nchunks = processor.process_pdf(\"example.pdf\")\n\n# Print structure\nfor chunk in chunks[:2]:\n    print(f\"\\nPage {chunk['metadata']['page']}:\")\n    print(chunk['text'][:100])\n</code></pre>"},{"location":"examples/pdf-processing/#advanced-features","title":"Advanced Features","text":""},{"location":"examples/pdf-processing/#1-table-detection","title":"1. Table Detection","text":"<pre><code>import tabula\n\ndef extract_tables(pdf_path):\n    \"\"\"Extract tables from PDF.\"\"\"\n    # Read tables\n    tables = tabula.read_pdf(\n        pdf_path,\n        pages='all',\n        multiple_tables=True\n    )\n\n    # Convert to text\n    table_texts = []\n    for i, table in enumerate(tables, 1):\n        text = f\"Table {i}:\\n\"\n        text += table.to_string()\n        table_texts.append(text)\n\n    return table_texts\n</code></pre>"},{"location":"examples/pdf-processing/#2-image-handling","title":"2. Image Handling","text":"<pre><code>from pdf2image import convert_from_path\nimport pytesseract\n\ndef extract_images(pdf_path):\n    \"\"\"Extract and OCR images from PDF.\"\"\"\n    # Convert PDF to images\n    images = convert_from_path(pdf_path)\n\n    # Process each image\n    image_texts = []\n    for i, image in enumerate(images, 1):\n        # OCR the image\n        text = pytesseract.image_to_string(image)\n\n        if text.strip():\n            image_texts.append({\n                'page': i,\n                'text': text.strip()\n            })\n\n    return image_texts\n</code></pre>"},{"location":"examples/pdf-processing/#best-practices","title":"Best Practices","text":"<ol> <li>Text Extraction</li> <li>Handle different PDF formats</li> <li>Preserve important whitespace</li> <li> <p>Remove artifacts carefully</p> </li> <li> <p>Chunking Strategy</p> </li> <li>Respect semantic boundaries</li> <li>Maintain context windows</li> <li> <p>Handle short sections</p> </li> <li> <p>Metadata</p> </li> <li>Track page numbers</li> <li>Preserve document structure</li> <li>Include source information</li> </ol>"},{"location":"examples/pdf-processing/#common-issues","title":"Common Issues","text":"<ol> <li>Poor Text Extraction</li> <li>Try different PDF libraries</li> <li>Check PDF encoding</li> <li> <p>Use OCR for scanned docs</p> </li> <li> <p>Layout Problems</p> </li> <li>Adjust parsing parameters</li> <li>Consider column detection</li> <li> <p>Handle headers/footers</p> </li> <li> <p>Special Content</p> </li> <li>Extract tables separately</li> <li>Process images with OCR</li> <li>Handle equations carefully</li> </ol>"},{"location":"examples/pdf-processing/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Custom Knowledge Base</li> <li>Learn about Evaluation</li> <li>See Deployment Options</li> </ul>"}]}